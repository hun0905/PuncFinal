{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/huan/.pyenv/versions/3.10.4/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.nn.functional import embedding\n",
    "from torch.utils import data\n",
    "from torch.utils.data import dataset\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import numpy as np\n",
    "from datasetBert import PuncDataset\n",
    "from datasetBert import Collate\n",
    "from datasetBILSTM import PuncDataset2\n",
    "from datasetBILSTM import Collate2\n",
    "from modelLSTM import BILSTM\n",
    "from modelBertLSTM import BertLSTMPunc\n",
    "from modelBert import BertPunc\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import  DataLoader,random_split,SubsetRandomSampler\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_size = 128 #定義鎖訓練的句子最大長度,超過會被切斷\n",
    "embedding_dim = 300 #embedding layer的size \n",
    "input_size = 37562#BILSMT用的vocabulary dictionary有多少詞彙\n",
    "pretrained = True#BILSTM是否用pretrained embedding layer\n",
    "hidden_size = 256#lstm 的hidden_size\n",
    "num_layers = 4#lstm有幾層\n",
    "save_path = '' #如果要繼續訓練之前的model,要繼續訓練的model放哪裡\n",
    "punc_path = 'punc.txt'#punctuation dictionary 的路徑\n",
    "data_path = 'data.txt'#training data 的路徑\n",
    "model_path = 'Bert' #要把model存在哪裡\n",
    "vocab_path = 'vocab.txt'\n",
    "model_num = 3#要用1.Bert或2.BertLSTM 3.BILSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定所用的dataset\n",
    "if model_num == 1 or model_num == 2:\n",
    "    dataset = PuncDataset(data_path,punc_path)\n",
    "else:\n",
    "    dataset = PuncDataset2(data_path,vocab_path,punc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train():\n",
    "    def __init__(self,dataset, model, criterion, optimizer,use_cuda,batch_size,epochs,scheduler,punc_path,load_path = 'punc_model',collate_fn = None,is_continue=False,\\\n",
    "                num_worker = 8,batch_size_times = 1,pin_memory = False,k = 10,with_l1= False,with_l2=False,l1_weight = 0,l2_weight=0):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.start = 0 #決定start 的epoch正常是0 但如果是載入model繼續訓練則會是過去所訓練到的epoch\n",
    "        self.epochs = epochs #epochs的上限 最多訓練幾epoch\n",
    "        self.dataset = dataset #資料 傳入的是PuncData的型態\n",
    "        self.epoch = 0 #目前所到的epcoh\n",
    "        self.model = model #所選的model 可能是 TBRNN或bi-lstm\n",
    "        self.criterion = criterion #所用的loss function 這裡是crossentropy\n",
    "        self.optimizer = optimizer #優化器，用以調整參數，這裡用adam\n",
    "        self.use_cuda = use_cuda #是否使用gpu\n",
    "        self.batch_size = batch_size #batch_size大小\n",
    "        self.scheduler = scheduler #學習率 decay 這裡用exp\n",
    "        self.load_path = load_path #如果要繼續訓練model,指定之前的model所在的path\n",
    "        self.collate_fn = collate_fn \n",
    "        self.is_continue = is_continue #重新訓練一個model或是繼續訓練之前的model\n",
    "        self.num_worker = num_worker #Dataloader的參數，正常時不用刻意調整\n",
    "        self.batch_size_times = batch_size_times #batch_size*batch_size_times就是真正的batch_size大小，因為gpu不足batch_size不能太大，要更大的batch_size時調整\n",
    "        self.pin_memory = pin_memory #Dataloader的參數,不用調整\n",
    "        self.with_l1 = with_l1 #是否添加l1正則化\n",
    "        self.with_l2 = with_l2 #是否添加l2正則化\n",
    "        self.l1_weight = l1_weight #l1正則化的weight\n",
    "        self.l2_weight = l2_weight #l2正則化的weight\n",
    "        with open(punc_path, encoding='utf-8') as file: \n",
    "            self.punc2id = { i + 1 : word.strip()for i, word in enumerate(file) } #建立index對punctuation的字典\n",
    "        self.punc2id[0] = \" \" #沒有標點\n",
    "        self.history_train_loss = [] #存每個epoch的train loss\n",
    "        self.history_val_loss = [] #存每個epoch的 val loss\n",
    "        \n",
    "        if is_continue: #是否是繼續訓練舊model\n",
    "            #載入舊model的狀態和各種參數\n",
    "            package = torch.load(load_path)\n",
    "            self.model = self.model.load_model(load_path).cuda()\n",
    "            for p in self.model.bert.parameters():\n",
    "                p.requires_grad = True\n",
    "            self.optimizer.load_state_dict(package['optim_dict'])\n",
    "            self.scheduler.load_state_dict(package['scheduler'])\n",
    "            self.start = package['epoch']\n",
    "            self.history_train_loss = package['train_loss']\n",
    "            self.history_val_loss = package['val_loss']\n",
    "        torch.manual_seed(42) \n",
    "        self.splits=KFold(n_splits=k,shuffle=True,random_state=42) #將整個train dataset隨機分k份 ,k-1用來train , 一份用來validation\n",
    "    def prfs(self,train_trues,train_preds,total_loss): #計算和評估各種指標並輸出\n",
    "        precision, recall, fscore, support = score(train_trues, train_preds)#將label和predict比較，計算出各類別Precision,Recall,和F-score\n",
    "        accuracy = accuracy_score(train_trues, train_preds) #計算全部的accuracy,包含空白\n",
    "        print(\"Multi-class accuracy: %.2f\" % accuracy) #accuracy 的精確度\n",
    "        SPLIT = \"-\"*(12*4+3) #分隔線\n",
    "        print(SPLIT)#分隔線輸出\n",
    "        #f = lambda x : round(x, 2)\n",
    "\n",
    "        #輸出每個標點符號的各種指標評估結果\n",
    "        for (v, k) in sorted(self.punc2id.items(), key=lambda x:x[1]):\n",
    "            if v >= len(precision): continue\n",
    "            if k == \" \":\n",
    "                k = \"  \"\n",
    "                continue\n",
    "            print(\"Punctuation: {} Precision: {:.3f} Recall: {:.3f} F-Score: {:.3f}\".format(k,precision[v],recall[v],fscore[v]))\n",
    "        print(SPLIT)\n",
    "\n",
    "        #計算和印出overall(總和不分類別)的所有指標\n",
    "        sklearn_accuracy = accuracy_score(train_trues, train_preds) \n",
    "        sklearn_precision = precision_score(train_trues, train_preds, average='micro')\n",
    "        sklearn_recall = recall_score(train_trues, train_preds, average='micro')\n",
    "        sklearn_f1 = f1_score(train_trues, train_preds, average='micro')\n",
    "        print(\"[sklearn_metrics] Total Epoch:{} loss:{:.4f} accuracy:{:.4f} precision:{:.4f} recall:{:.4f} f1:{:.4f}\".format(self.epoch+1, \\\n",
    "            total_loss, sklearn_accuracy, sklearn_precision, sklearn_recall, sklearn_f1))\n",
    "    def train_epoch(self,data_loader):\n",
    "        self.model.train()  #確保layers of model 在train mode\n",
    "        total_loss = 0\n",
    "        train_preds = [] #存放model預估的標點\n",
    "        train_trues = [] #存放label的真實標點\n",
    "        for  i,(data) in enumerate(data_loader):\n",
    "            #print(i)\n",
    "            input ,segment, label = data#輸入的資料(文字換成index),句子長度，label(標點的index)\n",
    "            #print(segment)\n",
    "            if  self.use_cuda:\n",
    "                input = input.cuda()\n",
    "                label = label.cuda()\n",
    "                segment = segment.cuda()\n",
    "                input = input.to(self.device)\n",
    "                label = label.to(self.device)\n",
    "                segment = segment.to(self.device)\n",
    "            outputs = self.model(input,segment)#將資料輸入model(調用model的forward),outputs為評估結果\n",
    "            #將outputs和label的dimension轉換，在用crossentropy評估loss\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            if self.use_cuda:\n",
    "                outputs = outputs.to(self.device)\n",
    "            label = label.view(-1)\n",
    "            loss = self.criterion(outputs, label)\n",
    "            loss_with_reg = loss#loss_with_reg是有加入正則化的loss，如果沒加就和loss相等\n",
    "            if self.use_cuda:\n",
    "                loss_with_reg = loss_with_reg.to(self.device)\n",
    "            if self.with_l1: #l1正則化\n",
    "                l1 = 0\n",
    "                l1 += sum ( [p.abs().sum() for p in self.model.encoder.parameters()] )\n",
    "                l1 += sum ( [p.abs().sum() for p in self.model.decoder.parameters()] )\n",
    "                l1 += sum ( [p.abs().sum() for p in self.model.projected.parameters()] )\n",
    "                l1_penalty = self.l1_weight *l1\n",
    "                loss_with_reg += l1_penalty\n",
    "            if self.with_l2: #l2正則化\n",
    "                l2 = 1e-3\n",
    "                l2 += sum ( [(p**2).sum() for p in self.model.encoder.parameters()] )\n",
    "                l2 += sum ( [(p**2).sum() for p in self.model.decoder.parameters()] )\n",
    "                l2 += sum ( [(p**2).sum() for p in self.model.projected.parameters()] )\n",
    "                l2_penalty = self.l2_weight *l2\n",
    "                loss_with_reg += l2_penalty\n",
    "            loss_with_reg.backward()#更新梯度\n",
    "            clipping_value = 2 # arbitrary value of your choosing\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), clipping_value)\n",
    "            if (i+1) % self.batch_size_times == 0 or (i+1) == len(data_loader):\n",
    "                self.optimizer.step() #計算weight\n",
    "                self.optimizer.zero_grad() #將梯度清空\n",
    "            total_loss += loss.item()\n",
    "            train_outputs = outputs.argmax(dim=1) #outputs原輸出的是四種class的機率分佈,換成最高機率class的index\n",
    "            pad_idx = np.where( label.detach().cpu().numpy() == 4)#[0][0]\n",
    "            if pad_idx[0].size == 0:\n",
    "                pad_idx = -1\n",
    "            else:\n",
    "                pad_idx = pad_idx[0][0]\n",
    "            train_preds.extend(train_outputs.detach().cpu().numpy()[:pad_idx])\n",
    "            train_trues.extend(label.detach().cpu().numpy()[:pad_idx])\n",
    "        self.scheduler.step()#進行learning rate decay\n",
    "        print('train: ','\\n')\n",
    "        self.prfs(train_trues,train_preds,total_loss)#印出這個epoch的train的結果評估\n",
    "        return total_loss/(i+1)\n",
    "    def val_epoch(self,data_loader):\n",
    "        val_loss = 0\n",
    "        self.model.eval()\n",
    "        #後面大致跟train epoch差不多\n",
    "        val_preds = []\n",
    "        val_trues = []\n",
    "        for i,(data) in enumerate(data_loader):\n",
    "            input , segment , label = data\n",
    "            if  self.use_cuda:\n",
    "                input = input.cuda()#換成可傳入gpu的型態\n",
    "                label = label.cuda()\n",
    "                segment = segment.cuda()\n",
    "                input = input.to(self.device)\n",
    "                label = label.to(self.device)\n",
    "                segment = segment.to(self.device)\n",
    "            \n",
    "            outputs = self.model(input,segment)\n",
    "            outputs = outputs.view(-1, outputs.size(-1))\n",
    "            if self.use_cuda:\n",
    "                outputs = outputs.to(self.device)\n",
    "            label = label.view(-1)\n",
    "            loss = self.criterion(outputs, label)\n",
    "            val_loss += loss.item()\n",
    "            val_outputs = outputs.argmax(dim=1)\n",
    "            pad_idx = np.where( label.detach().cpu().numpy() == 4)#[0][0]\n",
    "            if pad_idx[0].size == 0:\n",
    "                pad_idx = -1\n",
    "            else:\n",
    "                pad_idx = pad_idx[0][0]\n",
    "            val_preds.extend(val_outputs.detach().cpu().numpy()[:pad_idx])\n",
    "            val_trues.extend(label.detach().cpu().numpy()[:pad_idx])\n",
    "        print(\"validation: \",'\\n') \n",
    "        self.prfs(val_trues,val_preds,val_loss) #印出valdation 結果的評估\n",
    "        return val_loss/(i+1)\n",
    "    def train(self):\n",
    "        for fold, (train_idx,val_idx) in enumerate(self.splits.split(np.arange(len(self.dataset)))):\n",
    "            #train_idx 是被選為train data的資料的idx val_idx 是 val_data的資料的idx\n",
    "            train_sampler = SubsetRandomSampler(train_idx)#定義train的取樣方式，決定train要取哪些資料\n",
    "            val_sampler = SubsetRandomSampler(val_idx)#決定val要取哪些資料\n",
    "            train_loader = DataLoader(self.dataset, batch_size=self.batch_size, sampler=train_sampler,collate_fn=self.collate_fn,num_workers=self.num_worker,pin_memory=self.pin_memory)\n",
    "            val_loader = DataLoader(self.dataset, batch_size=self.batch_size, sampler=val_sampler,collate_fn=self.collate_fn,num_workers=self.num_worker,pin_memory=self.pin_memory)\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            print(device,'\\n')\n",
    "            #開始進入epoch，每一個epoch 都會經歷train epoch和val epoch\n",
    "            for epoch in range(self.start,self.epochs):\n",
    "                self.epoch = epoch\n",
    "                train_loss=self.train_epoch(train_loader)#回傳train loss\n",
    "                val_loss=self.val_epoch(val_loader)#回傳val loss\n",
    "                print(f\"Epoch:{self.epoch + 1} ; {self.epochs} average Training Loss:{train_loss} ; average Val Loss:{val_loss} \")\n",
    "                self.history_train_loss.append(train_loss)#將train loss 存入list\n",
    "                self.history_val_loss.append(val_loss) #將val loss 存入 list\n",
    "                #在checkpoint 將model儲存起來 要存取model和optimizer的state和epoch,history train及val loss語各種hyperparameter\n",
    "                #詳細部分可看Seq2Seq model 的 serialize\n",
    "                torch.save( self.model.serialize(self.model,self.optimizer,self.scheduler,epoch,self.history_train_loss,self.history_val_loss)\\\n",
    "                             ,model_path+str(self.epoch+1))   \n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) #計算model用了多少參數，可以大約估算model的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BILSTM(\n",
      "  (embedding): Embedding(37562, 300)\n",
      "  (lstm): LSTM(300, 256, num_layers=4, batch_first=True, bidirectional=True)\n",
      "  (linaer): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n",
      "parameters_count: 17144316\n"
     ]
    }
   ],
   "source": [
    "'''collate_fn 就是將調用 dataset中的getitem 所得到的資料進行拼接以我們要的形式輸出\n",
    "也就是在 for i,(data) in enumerate(data_loader) 當中 data所得到的資料就是拼接後的結果'''\n",
    "if model_num == 1 or model_num ==2:\n",
    "    collate_fn =Collate()\n",
    "else:\n",
    "    collate_fn =Collate2()\n",
    "'''\n",
    "BertLSTMPunc參數:\n",
    "segment是訓練的句子最大長度上限(以字為單位),output_size就是最後輸出的維度（標點數量＋1(空白)），num_layers是lstm的層數\n",
    ",p 是dropout rate的參數\n",
    "BertPunc參數:\n",
    "segment是訓練的句子最大長度上限(以字為單位),output_size就是最後輸出的維度（標點數量＋1(空白)）\n",
    ",p 是dropout rate的參數\n",
    "BILSTM參數:\n",
    "input_size輸入的dictionary大小 ,embedding_dim是embedding layer的size ，num_layers是lstm的層數\n",
    ",output_size就是最後輸出的維度（標點數量＋1(空白)）,pretrained是是否用pretrained embedding layer\n",
    "'''\n",
    "save_path = ''\n",
    "if model_num == 2:\n",
    "    model =BertLSTMPunc(segment_size,hidden_size,num_layers,4,0.3)\n",
    "elif model_num == 1:\n",
    "    model =BertPunc(segment_size,4,0.3)\n",
    "else:\n",
    "    model = BILSTM(input_size,embedding_dim,hidden_size,4,4,pretrained)\n",
    "if model_num == 1 or model_num == 2:\n",
    "    for p in model.bert.parameters():\n",
    "        p.requires_grad = True\n",
    "use_cuda = True\n",
    "if use_cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "print(model)\n",
    "print('parameters_count:',count_parameters(model))\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=4) #決定loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 2e-5,weight_decay=0.0)#決定optimizer （更新weight 的方式）\n",
    "scheduler = ExponentialLR(optimizer, gamma=1) # weight decay的方式（非必要）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL = Train(dataset,model,criterion,optimizer,use_cuda,1,30,scheduler,punc_path,save_path,collate_fn,False,batch_size_times=2,num_worker=2)\n",
    "MODEL.train()\n",
    "'''\n",
    "Train(dataset, model, criterion, optimizer,use_cuda,batch_size,epochs,scheduler,punc_path,load_path = 'punc_model',collate_fn = None\n",
    ",is_continue=False,num_worker = 8,batch_size_times = 1,pin_memory = False,k = 10,with_l1= False,with_l2=False,l1_weight = 0,l2_weight=0\n",
    "\n",
    "dataset： 資料 傳入的是PuncData的型態\n",
    "model： 所選的model\n",
    "criterion： 所用的loss function 這裡是crossentropy\n",
    "use_cuda： 是否使用gpu\n",
    "batch_size： batch_size大小\n",
    "epochs： epochs的上限 最多訓練幾epoch\n",
    "scheduler： 學習率 decay 這裡用exp \n",
    "punc_path : 標點符號字典路徑\n",
    "collate_fn : 選擇的collate_fn方式 此處用維我們自定義的collate_fn詳見dataset\n",
    "is_continue : 重新訓練一個model或是繼續訓練之前的model\n",
    "num_worker : Dataloader的參數，正常時不用刻意調整\n",
    "batch_size_times : batch_size*batch_size_times就是真正的batch_size大小，因為gpu不足batch_size不能太大，要更大的batch_size時調整\n",
    "pin_memory : Dataloader的參數,不用調整\n",
    "k #k的折數是多少\n",
    "with_l1 : 是否添加l1正則化\n",
    "with_l2 : 是否添加l2正則化\n",
    "l1_weight : l1正則化的weight\n",
    "l2_weight : l2正則化的weight\n",
    "'''\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('3.10.4': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cea872f7920547b6418835919bca84dc8673f29df8377ab241a7ca971c0dc1a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
